{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "BASEDIR = \"/work3/s204163/wiki/data-batches/\"\n",
    "cutoff_percentile = 98\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is used to filter the data, by cutting off the top percentile in order to remove outliers. Notice that it also deletes all references to the removed data in the other files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff:  11.0\n",
      "Group 1:  1390098\n",
      "Group 2:  28922\n"
     ]
    }
   ],
   "source": [
    "user_path = \"/work3/s204163/wiki/data-batches/users.pickle\"\n",
    "\n",
    "\n",
    "with open(user_path, \"rb\") as f:\n",
    "    users = pickle.load(f)\n",
    "    \n",
    "user_n_articles = {str(user.user_id): int(len(user.article_ids)) for user in users.values()}\n",
    "\n",
    "# Divide users into two groups, based on number of articles\n",
    "# First group should consists of the bottom 99 percentile of users\n",
    "# Second group should consists of the top 1 percentile of users\n",
    "\n",
    "cutoff = np.percentile(list(user_n_articles.values()), cutoff_percentile)\n",
    "\n",
    "group_keep = [user for user, n_articles in user_n_articles.items() if n_articles < cutoff]\n",
    "group_discard = [user for user, n_articles in user_n_articles.items() if n_articles >= cutoff]\n",
    "\n",
    "print(\"Cutoff: \", cutoff)\n",
    "print(\"Group 1: \", len(group_keep))\n",
    "print(\"Group 2: \", len(group_discard))\n",
    "\n",
    "user_keep = {user_id: users[user_id] for user_id in group_keep}\n",
    "user_discard = {user_id: users[user_id] for user_id in group_discard}\n",
    "\n",
    "\n",
    "with open(BASEDIR + f\"users_keep_cutoff{cutoff_percentile}.pickle\", \"wb\") as f:\n",
    "    pickle.dump(user_keep, f)\n",
    "\n",
    "with open(BASEDIR + f\"users_discard_cutoff{cutoff_percentile}.pickle\", \"wb\") as f:\n",
    "    pickle.dump(user_discard, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff:  1443.0600000000013\n",
      "Group 1:  19649\n",
      "Group 2:  401\n"
     ]
    }
   ],
   "source": [
    "article_path = \"/work3/s204163/wiki/data-batches/articles.pickle\"\n",
    "\n",
    "with open(article_path, \"rb\") as f:\n",
    "    articles = pickle.load(f)\n",
    "    \n",
    "article_n_users = {str(article.article_id): int(len(article.user_ids)) for article in articles.values()}\n",
    "\n",
    "# Divide users into two groups, based on number of users\n",
    "# First group should consists of the bottom 99 percentile of articles\n",
    "# Second group should consists of the top 1 percentile of articles\n",
    "\n",
    "cutoff = np.percentile(list(article_n_users.values()), cutoff_percentile)\n",
    "\n",
    "group_keep = [article for article, n_users in article_n_users.items() if n_users < cutoff]\n",
    "group_discard = [article for article, n_users in article_n_users.items() if n_users >= cutoff]\n",
    "\n",
    "print(\"Cutoff: \", cutoff)\n",
    "print(\"Group 1: \", len(group_keep))\n",
    "print(\"Group 2: \", len(group_discard))\n",
    "\n",
    "article_keep = {article_id: articles[article_id] for article_id in group_keep}\n",
    "article_discard = {article_id: articles[article_id] for article_id in group_discard}\n",
    "\n",
    "\n",
    "with open(BASEDIR + f\"articles_keep_cutoff{cutoff_percentile}.pickle\", \"wb\") as f:\n",
    "    pickle.dump(article_keep, f)\n",
    "    \n",
    "with open(BASEDIR + f\"articles_discard_cutoff{cutoff_percentile}.pickle\", \"wb\") as f:\n",
    "    pickle.dump(article_discard, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASEDIR + f\"users_keep_cutoff{cutoff_percentile}.pickle\", \"rb\") as f:\n",
    "    users = pickle.load(f)\n",
    "\n",
    "with open(BASEDIR + f\"articles_keep_cutoff{cutoff_percentile}.pickle\", \"rb\") as f:\n",
    "    articles = pickle.load(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users:  1390098\n",
      "Articles:  19649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19649/19649 [00:01<00:00, 19536.60it/s]\n",
      "100%|██████████| 1390098/1390098 [00:01<00:00, 789934.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users:  982463\n",
      "Articles:  17963\n",
      "/work3/s204163/wiki/data-batches/users_keep_cutoff98_sync.pickle\n",
      "/work3/s204163/wiki/data-batches/articles_keep_cutoff98_sync.pickle\n"
     ]
    }
   ],
   "source": [
    "print(\"Users: \", len(users))\n",
    "print(\"Articles: \", len(articles))\n",
    "\n",
    "for article in tqdm(articles.values(), total=len(articles)):\n",
    "    article.user_ids = [user_id for user_id in article.user_ids if user_id in users.keys()]\n",
    "\n",
    "articles_sync = {article.article_id: article for article in articles.values() if len(article.user_ids) > 0}\n",
    "\n",
    "for user in tqdm(users.values(), total=len(users)):\n",
    "    user.article_ids = [article_id for article_id in user.article_ids if article_id in articles.keys()]\n",
    "\n",
    "users_sync = {user.user_id: user for user in users.values() if len(user.article_ids) > 0}\n",
    "\n",
    "print(\"Users: \", len(users_sync))\n",
    "print(\"Articles: \", len(articles_sync))\n",
    "\n",
    "\n",
    "with open(BASEDIR + f\"users_keep_cutoff{cutoff_percentile}_sync.pickle\", \"wb\") as f:\n",
    "    pickle.dump(users_sync, f)\n",
    "\n",
    "with open(BASEDIR + f\"articles_keep_cutoff{cutoff_percentile}_sync.pickle\", \"wb\") as f:\n",
    "    pickle.dump(articles_sync, f)\n",
    "    \n",
    "print(BASEDIR + f\"users_keep_cutoff{cutoff_percentile}_sync.pickle\")\n",
    "print(BASEDIR + f\"articles_keep_cutoff{cutoff_percentile}_sync.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "982463\n",
      "17963\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "nodes = set()\n",
    "for article in articles_sync.values():\n",
    "    for user_id in article.user_ids:\n",
    "        nodes.add(user_id)\n",
    "print(len(nodes))\n",
    "\n",
    "unique_articles = set()\n",
    "for user in users_sync.values():\n",
    "    for article_id in user.article_ids:\n",
    "        unique_articles.add(article_id)\n",
    "print(len(unique_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
